---
title: "Lab 1"
output: pdf_document
author: "By Axel Holmberg (axeho681)"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Programming/TDDE01/Lab 1")
library(kknn)
```

# Assignment 1
## 1.
*Import the data into R and divide it into training and test sets (50%/50%)*
```{r Task1, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
data <- read.csv2("spambase.csv")

#Split data into training and test set.

n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
```

## 2.
*Use logistic regression (functions glm(), predict()) to classify the training and test data by the classification principle $\hat{Y}=1$ if $(Y=1|X) > 0.5$, otherwise $\hat{Y}=0$ and report the confusion matrices (use table()) and the misclassification rates for training and test data. Analyse the obtained results.*

```{r Task 2, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
model <- glm(Spam ~ ., family=binomial, data=train)
#GLM model for data with family binomial 
#--> only 0s and 1s

predictModel= predict(model, newdata=test, type="response") 

probability <- ifelse(predictModel > 0.5, "1", "0") 
#Split up the model into spam and not spam

confMatrix <- table(probability, test[,"Spam"]) #Confusionmatrix from the model

modelDiag <- diag(confMatrix) #Diagonal of the 

missClMa1 = 1-(sum(modelDiag)/sum(confMatrix)) 
#Missclassfication rate by dividing the diagonal 
#from the confusionmatricx with the whole confusionmatrix

#Same but for training data
predictModel_train = predict(model, newdata=train, type="response") 

probability_train <- ifelse(predictModel_train > 0.5, "1", "0") 
#Split up the model into spam and not spam

confMatrix_train <- table(probability_train, train[,"Spam"]) 
#Confusionmatrix from the model

modelDiag_train <- diag(confMatrix_train) #Diagonal of the 

missClMa1_train = 1-(sum(modelDiag_train)/sum(confMatrix_train)) 
#Missclassfication rate 
#by dividing the diagonal from the confusionmatricx with the whole confusionmatrix

#Prints results
print("Confusion matrix 2 test:")
print(confMatrix)
print("missclassification 2 test:")
print(missClMa1)

print("Confusion matrix 2 train:")
print(confMatrix_train)
print("missclassification 2 train:")
print(missClMa1_train)
```

##### Analyse the obtained results.
The confusion matrix gives us the misclassification rate. I would say that the misclassification rate is okay for its' aplication. It is all very dependent on the use case though. Also, the missclasification rate for the training and test is almost the same, which is really good.

## 3.
*Use logistic regression to classify the test data by the classification principle $\hat{Y}=1$ if $p(Y=1|X) > 0.8$, otherwise $\hat{Y}=0$ and report the confusion matrices (use table()) and the misclassification rates for training and test data. Compare the results. What effect did the new rule have?*

```{r}
#Split up the model into spam and not spam
probability2 <- ifelse(predictModel > 0.8, "1", "0") #Split up the model into spam and not spam

confMatrix2 <- table(probability2, test[,"Spam"]) 

modelDiag2 <- diag(confMatrix2)

missClMa2 = 1-(sum(modelDiag2)/sum(confMatrix2))

#Same but with training data
probability2_train <- ifelse(predictModel_train > 0.8, "1", "0") #Split up the model 
#into spam and not spam
confMatrix2_train <- table(probability2_train, train[,"Spam"]) #Confusionmatrix from the model
modelDiag2_train <- diag(confMatrix2_train) #Diagonal of the 

missClMa2_train = 1-(sum(modelDiag2_train)/sum(confMatrix2_train)) #Missclassfication 
#rate by dividing the diagonal from the confusionmatricx with the whole confusionmatrix


print("Confusion matrix 3:")
print(confMatrix2)
print("missclassification 3:")
print(missClMa2)

print("Confusion matrix 3 train:")
print(confMatrix2_train)
print("missclassification 3 train:")
print(missClMa2_train)
```

##### What effects did the new rule have?
The new rule made it so that the rate is a bit worse, so it should stay at the previous value. Regarding the difference in training and test data missclassification it is a bit worse than before as well as there is a larger difference between the two, which indicates some form of overfitting.


## 4.
*Use standard classifier kknn() with K=30 from package kknn, report the the misclassification rates for the training and test data and compare the results with step 2.*
```{r}
#KKNN with K=30

kknn_K30 = kknn(Spam ~ ., train=train, test=test, k=30)
kknn_K30_pred = predict(kknn_K30)

kknn_K30_pred <- ifelse(kknn_K30_pred > 0.5, 1, 0) 
#Split up the model into spam and not spam


confMa_K30 = table(kknn_K30_pred, test[,"Spam"])
misCl_K30 = 1-sum(diag(confMa_K30)/sum(confMa_K30))

#Training data
kknn_K30_train = kknn(Spam ~ ., train=train, test=train, k=30)
kknn_K30_pred_train = predict(kknn_K30_train)
kknn_K30_pred_train <- ifelse(kknn_K30_pred_train > 0.5, 1, 0) 
#Split up the model into spam and not spam

confMa_K30_train = table(kknn_K30_pred_train, train[,"Spam"])
misCl_K30_train = 1-sum(diag(confMa_K30_train)/sum(confMa_K30_train))

print("Missclassification 4 testing:")
print(misCl_K30)

print("Missclassification 4 training:")
print(misCl_K30_train)
```

##### Compare the results with step 2.
The misclassification rate is even worse than in step 2. Probably because of the realtively small dataset, which fits a parametric method better. If one would have more data then the K-nearest neighbour could be better for this application, as that works better for non-parametric methods. This also shows when comparing the test data and training data missclassification rates.

## 5.
*Repeat step 4 for $K=1$ and compare the results with step 4. What effect does the decrease of K lead to and why?*

```{r}
#KKNN with K=1
kknn_K1 = kknn(Spam ~ ., train=train, test=test, k=1)
kknn_K1_pred = predict(kknn_K1)
kknn_K1_pred <- ifelse(kknn_K1_pred > 0.5, 1, 0) #Split up the model into spam and not spam
confMa_K1 = table(kknn_K1_pred, test[,"Spam"])
misCl_K1 = 1-sum(diag(confMa_K1)/sum(confMa_K1))

#Training data
kknn_K1_train = kknn(Spam ~ ., train=train, test=train, k=1)
kknn_K1_pred_train = predict(kknn_K1_train)
kknn_K1_pred_train <- ifelse(kknn_K1_pred_train > 0.5, 1, 0) #Split up the model into spam and not spam
confMa_K1_train = table(kknn_K1_pred_train, train[,"Spam"])
misCl_K1_train = 1-sum(diag(confMa_K1_train)/sum(confMa_K1_train))

print("Missclassification 5 testing:")
print(misCl_K1)

print("Missclassification 5 training:")
print(misCl_K1_train)
```

##### What effect does the decrease of K lead to and why?
The decrease in K leads to a noisier prediction and is thereby worse. It can also lead to overfitting. For the training data it is a perfect fit with the training data as there is a lot of overfitting with k=1.

# Assignment 2
## 1.
*Import the data to R.*
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(readxl)

set.seed(12345)
machines <- read_excel("machines.xlsx")

```

## 2.
*Assume the probability model $x$ ($\mathrm{p}(x \mid \theta)=\theta e^{-\theta x}$) for x=Length in which observations are independent and identically distributed. What is the distribution type of x? Write a function that computes the log-likelihood log(p(x|theta)) for a gven theta and a given data vector x. Plot the curve showing the dependence of log-likelihood on theta where the entire data is used for fitting. What is the maximum likelihood value of theta according to the plot?*

```{r}
#Comes from  p(x|theta) = theta*exp(-theta*x) and then using thew likelihood 
#function with it and then minimizing that with 
#-log(L(theta))=-log((theta^n*exp(-theta*sum(x)). 
likelihoodlog = function(x, theta) {
  return(-dim(x)[1]*log(theta) + theta*sum(x))
}
print(likelihoodlog(machines[1], 1))

#Plot curve
curve(likelihoodlog(machines, x),xlim=c(0,4), ylim=c(0,60) , col="blue", 
      ylab="-log(likelihood(machines,theta))")

#Min theta. Comes from deriving the loglikelihood-function. 
#The more data I have the more exactly I can pinpoint the exact point of failure
minvalue = dim(machines)/sum(machines)
print("Min theta whole data set:")
minthetalikelihood = function(x) {
  return (dim(x)[1]/sum(x))
}
print(minthetalikelihood(machines))

```


## 3.
*Repeat step 2 but use only 6 first observations from the data, and put the two log-likelihood curves (from step 2 and 3) in the same plot. What can you say about reliability of the maximum likelihood solution in each case?*

```{r}
curve(likelihoodlog(machines, x),xlim=c(0,4), xlab="Theta", ylim=c(0,60) ,  col="red" , ylab="-log(likelihood(machines,theta))")
curve(likelihoodlog(machines[1:6,],x),xlim=c(0,4), ylim=c(0,60) , col="blue", add=TRUE)
```
Where the blue line is with all the data points and the red line is with only the first six data points.
```{r}
print("Min theta first 6 data points:")
print(minthetalikelihood(machines[1:6,]))
```

##### What can you say about reliability of the maximum likelihood solution in each case?
The maximum likelihood wolution for the larger dataset (the blue line) it a lot more defined and easier to see where the $min-\theta$ is. Also, the change in $\theta$ has more effect as it increases than it has on the the curve for the smaller data set(the red line). The $\theta$ also goes from $1.126217$ to $1.785681$.

## 4.
*Assume now a Bayesian model with $p(x|\theta)=\theta e^{-\theta x}$ and a prior $\mathrm{p}(\theta)=\lambda e^{-\lambda \theta}$, $\lambda=10$.Write a function computing $\mathrm{l}(\theta)=log(p(x|\theta)p(\theta))$. What kind of measure is actually computed by this function? Plot the curve showing the dependence of $l(\theta)$ on theta computed using the entire data and overlay it with a plot from step 2. Find an optimal theta and compare your result with the previous findings.*

```{r}
bayesianfunc = function(x, theta, lambda) {
  return(likelihoodlog(x, theta) - log(lambda) + lambda*theta)
} 

curve(likelihoodlog(machines, x),xlim=c(0,4), xlab="Theta",ylab="-log(likelihood(machines,theta))", ylim=c(0,60) ,  col="red")
curve(likelihoodlog(machines[1:6,],x),xlim=c(0,4), ylim=c(0,60) , col="blue", add=TRUE)
curve(bayesianfunc(machines,x,10), xlab="theta", ylab="l(theta)", xlim=c(0,4), 
      ylim=c(0,60), col="green", add=TRUE)

#min theta for bayesianfunc. I get it from deriving the bayesianfunc 
#with respect to theta and set it to = 0 to get the min.
print("Min theta bayesian func.:")
print(dim(machines)[1]/(sum(machines)+10))
```

##### Compare your results with the previous findings
The same can be said again regarding the blue and red line as was said in the 4. The added green line is even more defined and the increase in $\theta$ has an even greater effect than on the red and blue line. The value of $\theta$ is also the lowest at $0.91271907$ compared to $1.126217$ for the red line and $1.785681$ for the blue line.

## 5.
*Use theta value found in step 2 and generate 50 new observations from $p(x|\theta)=\theta e^{-\theta x}$ (use standard random number generators). Create the histograms of the original and the new data and make conclusions.*
```{r}
theta = minthetalikelihood(machines)

##Creaes 5 new data points with the rate from 2.
newdata = rexp(50, rate=theta)
print(newdata)
#Stores the old data from the Length col in the machines
olddata <- machines$Length

#Creages new window for plots
hist(olddata, col="red", xlim=c(0,5), ylim=c(0,20), xlab="x", 
     main="Histogram of the original and new data")
hist(newdata, col="blue", xlim=c(0,5), ylim=c(0,20), add=TRUE, breaks="FD", xlab="x")
```
The red stacks are of the old data, and the blue stacks are of the new generated data.

##### Conclusions
The histogram shows that the generated data based on the chosen $\theta$ actually fits the old data quite well and the model is thereby fitting.


# Assignment 4
```{r echo=FALSE}
library(readxl)
library(Metrics)
library(MASS)
library(ggplot2)
library(glmnet)

set.seed(12345)

```

## 1.
*Import data to R and create a plot of Moisture versus Protein. Do you think that these data are described well by a linear model?*
```{r}
#Imports file
tecator <- read_excel("tecator.xlsx")
moisture = tecator$Moisture;
protein = tecator$Protein;

#Plots the moisture and protein
plot(protein, moisture, xlab="Moisture", ylab="Protein", main="Moisture vs. Protein")

#Quite well with a linear model. Prob quite large deviations sometimes though

#Below is just to see the linear model
data_moisture_protein = tecator[,103:104]

fit1 <- lm(formula = moisture ~ protein, data=data_moisture_protein)
#Gives the 
fitted1 = predict(fit1, interval="confidence")
#Plots line with protein as x and the new predicted fitted values as y
lines(protein, fitted1[, "fit"])
```

##### Do you think that these data are described well by a linear model?
Yes, it can be described quite well as can be seen in the picture above. There are a few outliers though.

## 2.
*Consider model $M_i$ in which Moisture is normally distributed, and the expected Moisture is a polynomial function of Protein including the polynomial terms up to power $i$(i.e M1 is a linear model, M2 is a quadratic model and so on). Report a probabilistic model that describes $M_i$ . Why is it appropriate to use MSE criterion when fitting this model to a training data?*


Consider the functions are: $M_1 = w_0 + w_1*x_1 + e$, $M_2 = w_0 + w_1*x_1 + w_2*x_2^2 + e$ and so on. These functiions are probabalistic are described via regression. It is thereby appropriate to use MSE to get the error for the model.


## 3.
*Divide the data into training and validation sets( 50%/50%) and fit models $M_i, i=1 ...6$. For each model, record the training and the validation MSE and present a plot showing how training and validation MSE depend on i (write some R code to make this plot). Which model is best according to the plot? How do the MSE values change and why? Interpret this picture in terms of bias-variance tradeoff.*

```{r echo=TRUE}
#Splits the data into training set and test set using only moisture and protein cols
n=dim(tecator[,103:104])
set.seed(12345)
id=sample(1:n[1], floor(n*0.5))
train=tecator[id,103:104]
test=tecator[-id,103:104]

#Splits up the test and training data to protein and moisture respectively
p_train = train$Protein
m_train = train$Moisture
p_test = test$Protein
m_test = test$Moisture


#Models below for M1-M6, regression
m1_model = lm(formula = Moisture ~ Protein, data=train)
m2_model = lm(formula = Moisture ~ Protein + I(Protein^2), data = train)
m3_model = lm(formula = Moisture ~ Protein + I(Protein^2) + 
                I(Protein^3), data = train)
m4_model = lm(formula = Moisture ~ Protein + I(Protein^2) + 
                I(Protein^3) + I(Protein^4), data = train)
m5_model = lm(formula = Moisture ~ Protein + I(Protein^2) + 
                I(Protein^3) + I(Protein^4) + I(Protein^5), data = train)
m6_model = lm(formula = Moisture ~ Protein + I(Protein^2) + 
                I(Protein^3) + I(Protein^4) + I(Protein^5) + I(Protein^6), data = train)

#Predictions with the help of the training data
m1_model_pred = predict(m1_model, newdata=train)
m2_model_pred = predict(m2_model, newdata=train)
m3_model_pred = predict(m3_model, newdata=train)
m4_model_pred = predict(m4_model, newdata=train)
m5_model_pred = predict(m5_model, newdata=train)
m6_model_pred = predict(m6_model, newdata=train)

#Predictions with the help of the test data
m1_model_pred_test = predict(m1_model, newdata=test)
m2_model_pred_test = predict(m2_model, newdata=test)
m3_model_pred_test = predict(m3_model, newdata=test)
m4_model_pred_test = predict(m4_model, newdata=test)
m5_model_pred_test = predict(m5_model, newdata=test)
m6_model_pred_test = predict(m6_model, newdata=test)


#Mean squared error for the training data
mse_train <- vector()
mse_train[1] <- mse(m_train, m1_model_pred)
mse_train[2] <- mse(m_train, m2_model_pred)
mse_train[3] <- mse(m_train, m3_model_pred)
mse_train[4] <- mse(m_train, m4_model_pred)
mse_train[5] <- mse(m_train, m5_model_pred)
mse_train[6] <- mse(m_train, m6_model_pred)

#Mean squared error for the testing data
mse_test <- vector()
mse_test[1] <- mse(m_test, m1_model_pred_test)
mse_test[2] <- mse(m_test, m2_model_pred_test)
mse_test[3] <- mse(m_test, m3_model_pred_test)
mse_test[4] <- mse(m_test, m4_model_pred_test)
mse_test[5] <- mse(m_test, m5_model_pred_test)
mse_test[6] <- mse(m_test, m6_model_pred_test)

plot(1:6, mse_train, col="blue", type="l", xlab="i", ylab="MSE", 
     ylim=c(23,45), main="MSE dependencie of i. Where green = test, blue= train")
lines(1:6, mse_test,col="green")
```

##### Which model is best according to the plot? How do the MSE values change and why? Interpret this picture in terms of bias-variance tradeoff.
The best model is when i = 3 as it has the lowest MSE for both the test data and training data. The MSE for the training data has even decreased, and the MSE for the training data is constant. For $i\geq4$ the MSE increases for the testing data due to overfitting.

## 4.
*Perform variable selection of a linear model in which Fat is response and Channel1-Channel100 are predictors by using stepAIC. Comment on how many variables were selected.*
```{r include=FALSE}
fitted_fat <- lm(tecator$Fat ~ ., data=tecator[,2:101])
steps <- stepAIC(fitted_fat, direction="both")
```
````{r}
print("Number of selected variables:")
print(length(steps$coefficients)-1) 
`````
There are 63 variables chosen from the stepAIC method.

## 5.
*Fit a Ridge regression model with the same predictor and response variables. Present a plot showing how model coefficients depend on the log of the penalty factor $\lambda$ and report how the coefficients change with $\lambda$.*
```{r}
#Takes the scaled tecator of Channel1-100
covariates=scale(tecator[,2:101])
#Scales the response-variable, which in this case is the fat
response=scale(tecator[,102])
#Using glmnet with alpha=0 gives the Ridge-Regression
model_ridge=glmnet(as.matrix(covariates), response, alpha=0,family="gaussian")
plot(model_ridge, xvar="lambda", label=TRUE, main="Ridge Regression\n")
```

The coefficents all tend to zero as lambda increases.

## 6.
*Repeat step 5 but fit LASSO instead of the Ridge regression and compare the plots from steps 5 and 6. Conclusions?*
```{r}
model_lasso = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian")
plot(model_lasso, xvar="lambda", label=TRUE, main="Lasso Regression\n")
```

##### Conclusions
In both the plots in step 5 and 6 the coefficients tend to zero as lambda increases. The big difference is how they get there, as can be seen in the graphs. In Ridge regression the $\lambda$ parameter minimizes the impact of the coefficients to be lower, which gives the smoother lines as it tends to zero. Although, with LASSO-regression the coefficents are instead set to absolute zero, which gives the more jagged line in the graph as it tends to zero. 


## 7.
*Use cross-validation to find the optimal LASSO model (make sure that case $\lambda = 0$ is also considered by the procedure) , report the optimal $\lambda$ and how many variables were chosen by the model and make conclusions. Present also a plot showing the dependence of the CV score and comment how the CV score changes with $\lambda$.*

```{r}
cv_lasso_model = cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian", lambda=c(model_lasso$lambda,0))
plot(cv_lasso_model)
print("Min lambda cv :")
print(cv_lasso_model$lambda.min)
```

##### Answers for 7. and 8.

*Report the optimal $\lambda$ and how many variables were chosen by the model and make conclusions. Compare the results from step 4 and 7.*

As can be seen in the graph above the optimal $\lambda$ is $0$. This means that the optimal amount of features to keep is all 100 of them and that according to CV with Lasso that all of the variables are required to minimize the MSE. This is a lot higher compared to the 63 variables kept in step 4 in the stepAIC-method.

