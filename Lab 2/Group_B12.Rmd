---
title: "Lab 2 - Group B 12"
author: Axel Holmberg (axeho681), Jonathan Reimertz (jonre639) and Wilhelm Hansson
  (wilha431)
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	warning = FALSE
)
```


```{r setup_and_func_def, echo=FALSE, warning = FALSE}
# This code is for setup and function definition. It will not be printed to the output file.
# IMPORTS
library(ggplot2)
library(MASS)
library(tree)
library(readxl)
library(e1071)
library(knitr) # For making nice tables

# Set random generator
set.seed(12345, sample.kind="Rounding")
```

# Assignment 1. LDA and logistic regression
In this assignment we work with a data set containing information about  Austrailian crabs. The first task is to plot carapace length (CL) versus rear width (RW) and group data by Sex.

```{r scatter_plot_observed, echo=FALSE}
crabs.data <- read.csv("australian-crabs.csv")
crabs.sex.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=sex)) + 
  geom_point() + 
  labs(title="carapace length (CL) versus rear width (RW)", subtitle="Observed sex used for coloring data.")
crabs.sex.plot

```

As can be observed in the plot. A line can be drawn, dividing the data in to two groups. Therefore, linear discriminant analysis can be used for modeling and making preditions.

Based on this conclusion a LDA analysis with target Sex and features CL and RW was made and a model derived using `lda()` with a proportional prior. After the model was derived it was used to predict the Sex of crabs in the data set. The prediction was then used to color the intial data and make a plot similar to the previous one.

```{r scatter_plot_predicted, echo=FALSE}
crabs.sex.lda_model <- lda(sex ~ RW + CL, data = crabs.data)
crabs.predicted <- predict(crabs.sex.lda_model,crabs.data, prior = crabs.sex.lda_model$prior)
crabs.predicted.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=crabs.predicted$class)) +
  geom_point() +
  labs(color="sex", 
       title="carapace length (CL) versus rear width (RW).", 
       subtitle="Predicted sex used for coloring data. Proportional prior")
crabs.predicted.plot
```

One can observe a small difference in the coloring of the data points. These changes occur close to the tail of the data ploted data, meaning smaller crabs. Another observation is that the the border between the different groups has become more distinct. The reason being that we use LDA when fitting out model.

To see how good of a prediction the model did we create a confusion matrix and calculate missclassification rate.
```{r echo=FALSE}
crabs.predicted.confusionmatrix <- table(predicted=crabs.predicted$class,observed=crabs.data$sex)
crabs.predicted.missclassification_rate <- 1- (sum(diag(crabs.predicted.confusionmatrix))/sum(crabs.predicted.confusionmatrix))

print("Confusion matrix")
crabs.predicted.confusionmatrix
print(paste("Missclassification rate:", crabs.predicted.missclassification_rate))
```
With this missclassification rate the model can be said to be of good quality. However considiring the size of the data set nothing can be assumed about the over all quality.

Now the priors will be changed from $p(Male)=p(Female)=0.5$ to $p(Male)= 0.9 \quad and \quad p(Female)=0.1$ and the confusion matrix and missclassification rate is once more calculated. Also the data points are once more ploted, now using the new prediction for coloring.

```{r echo=FALSE}
crabs.sex.own_priors <- crabs.sex.lda_model$prior
crabs.sex.own_priors <- c(0.1,0.9)
crabs.sex.own_priors.predicted <- predict(crabs.sex.lda_model,crabs.data, prior = crabs.sex.own_priors)
crabs.sex.own_priors.predicted.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=crabs.sex.own_priors.predicted$class)) + 
  geom_point() +
  labs(color="sex", 
       title="carapace length (CL) versus rear width (RW).", 
       subtitle="Predicted sex used for coloring data. Own prior")
crabs.sex.own_priors.predicted.plot

crabs.sex.own_priors.predicted.confusionmatrix <- table(predicted=crabs.sex.own_priors.predicted$class,observed=crabs.data$sex)
crabs.sex.own_priors.predicted.missclassification_rate <- 1- (sum(diag(crabs.sex.own_priors.predicted.confusionmatrix))/sum(crabs.sex.own_priors.predicted.confusionmatrix))

print("Confusion matrix")
crabs.sex.own_priors.predicted.confusionmatrix
print(paste("Missclassification rate:", crabs.sex.own_priors.predicted.missclassification_rate))
```

As the output shows, the missclassification rate has increased. This can also be observed when comparing this plot with the previous one. Changing the prior from a proportional to $p(Male)=p(Female)=0.5$ to $p(Male)= 0.9 \quad and \quad p(Female)=0.1$ means that we "tell" the model that previous observations show this distribution between the two sexes.

The last step of this assignmen is to use `glm()`to perform a logistic regression. The data will once more be ploted but now also with the line creating the decisions boundry.

```{r glm, echo=FALSE, warning=FALSE}
crabs.sex.glm_model <- glm(sex ~ RW + CL, family=binomial, data = crabs.data)
crabs.sex.glm_model.predicted <- predict(crabs.sex.glm_model,crabs.data, type="response")
crabs.sex.glm_model.classified <- ifelse(crabs.sex.glm_model.predicted > 0.5, "Male", "Female")

crabs.sex.glm_model.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=crabs.sex.glm_model.classified)) +
  geom_point() +
  labs(color="sex", 
       title="carapace length (CL) versus rear width (RW).", 
       subtitle="Predicted sex used for coloring data. Logistic regression")

# Extracting coefficients for line equation
line.B = crabs.sex.glm_model[["coefficients"]][["CL"]]
line.A = crabs.sex.glm_model[["coefficients"]][["RW"]]
line.M = crabs.sex.glm_model[["coefficients"]][["(Intercept)"]]
line.limit = 0 # From the equation of a straight line
# Creating function from line equation to use for plot
line.equ <- function(x){(line.B*x+line.M-line.limit)/(-line.A)}

crabs.sex.glm_model.plot <- crabs.sex.glm_model.plot + 
  geom_line(aes(x = CL, y = line.equ(CL), color = "Decision\nboundry"))

crabs.sex.glm_model.plot

# conf and miss

crabs.sex.glm_model.predicted.confusionmatrix <- table(predicted=crabs.sex.glm_model.classified,observed=crabs.data$sex)
crabs.sex.glm_model.predicted.missclassification_rate <- 1- (sum(diag(crabs.sex.glm_model.predicted.confusionmatrix))/sum(crabs.sex.glm_model.predicted.confusionmatrix))

```

```{r results= 'asis', echo=FALSE}
    cat("The equation for the line is: ")
    cat("RW =",line.B/(-line.A),"* CL +",(line.M-line.limit)/(-line.A))
    cat("\n\n")
```

Below the confusion matrix and missclassificaation rate for the logistic regression classification can be found.

```{r echo=FALSE, warning=FALSE}
print("Confusion matrix")
crabs.sex.glm_model.predicted.confusionmatrix
print(paste("Missclassification rate:", crabs.sex.glm_model.predicted.missclassification_rate))


```


Last the missclassification rates for each model is compared.
```{r, echo=FALSE}
dft <- data.frame(matrix(c("LDA","Proportional",crabs.predicted.missclassification_rate,"LDA","0.9/0.1",crabs.sex.own_priors.predicted.missclassification_rate,"GLM","Proportional",crabs.sex.glm_model.predicted.missclassification_rate),nrow=3,ncol=3, byrow=TRUE))
colnames(dft) <- c("type","prior","missclassification rate")
kable(dft, format = "latex")
```
As one can observe LDA and GLM give the same error rate when proportional priors are used.


\newpage

# Assignment 2

```{r}
data <- read_excel("creditscoring.xls")
set.seed(12345, sample.kind="Rounding")

#Splits data into training, validation and test
n=dim(data)[1]
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345, sample.kind="Rounding")
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
```


## Step 1 - Import the data

The data is imported and divided into $train=50\%, validation=25\%, test=25\%$.

## Step 2 - Tree for deviance and gini

```{r include=FALSE}
#Train and test for deviance
fit.train.dev <- tree(ifelse(good_bad == "good", 1, 0) ~ . , data=train, split="deviance")

#Train and test for gini
fit.train.gini <- tree(ifelse(good_bad == "good", 1, 0)  ~ . , data=train, split="gini")

#Predictions
fit_pred.train.dev <- predict(fit.train.dev, newdata=train)

fit_pred.train.gini <- predict(fit.train.gini, newdata=train)

fit_pred.test.dev <- predict(fit.train.dev, newdata=test)

fit_pred.test.gini <- predict(fit.train.gini, newdata=test)

#Conf matrix
confMa.train.dev <- table(train$good_bad,(ifelse(fit_pred.train.dev > 0.5, "good", "bad")))
confMa.test.dev <- table(test$good_bad,(ifelse(fit_pred.test.dev > 0.5, "good", "bad")))
confMa.train.gini <- table(train$good_bad,(ifelse(fit_pred.train.gini > 0.5, "good", "bad")))
confMa.test.gini <- table(test$good_bad,(ifelse(fit_pred.test.gini > 0.5, "good", "bad")))

#Misclassification rates
misCl.train.dev <- 1-(sum(diag(confMa.train.dev))/sum(confMa.train.dev))
misCl.test.dev <- 1-(sum(diag(confMa.test.dev))/sum(confMa.test.dev))
misCl.train.gini <- 1-(sum(diag(confMa.train.gini))/sum(confMa.train.gini))
misCl.test.gini <- 1-(sum(diag(confMa.test.gini))/sum(confMa.test.gini))

#Prints the miscl. rates
misCl.train.dev
misCl.test.dev
misCl.train.gini
misCl.test.gini
```

The missclassifiaction rates are `r misCl.train.dev` for deviance and `r misCl.train.gini` for gini with training data and `r misCl.test.dev` for deviance and `r misCl.test.gini` for gini with test data.

## Step 3 - Optimal tree

```{r}
trainScore=rep(0,9)
validationScore=rep(0,9)

for (i in 2:9) {
  set.seed(12345, sample.kind="Rounding")

  prunedTree <- prune.tree(fit.train.dev, best=i)
  prediction <- predict(prunedTree, newdata = valid, type = "tree") # Predict with the pruned tree and val set
  trainScore[i] <- deviance(prunedTree) # Calculate deviance of test set
  validationScore[i] <- deviance(prediction) # Calculate deviance of val set
}

plot(2:9, trainScore[2:9], type="b", col="green", ylim=c(40,100), ylab="Deviance", xlab="No. of leaves")
points(2:9, validationScore[2:9], type="b", col="blue", ylim=c(40,100))
legend("top", legend=c("Training score (green)", "Validation score (blue)"))
```

The graph above shows the deviance for the training score and the validation score. As one can see the optimal number of leaves would be 5 or 6 as they are at the same value. 


```{r include=FALSE}
#Optimal number of leaves
match(min(validationScore[2:9]),validationScore)
optPrunedTree <- prune.tree(fit.train.dev, best=match(min(validationScore[2:9]),validationScore))
```


```{r}
plot(optPrunedTree, sub="asd")
text(optPrunedTree, pretty=0)
title("Pruned tree with 6 leaves")

```



```{r}

#Estimates classification 
predict.tree <- predict(optPrunedTree, newdata = test)
confMa.prune <- table(ifelse(predict.tree>0.5, "good", "bad"), test$good_bad)
testMisClass <- 1-sum(diag(confMa.prune)/sum(confMa.prune))

```

In the tree above best is set to 5, but it chooses the next best tree if that doesn't work, which is with 6 leaves. Tee value for 6 is the same as 5 so that should work just as well. The missclassification rate of the tree is `r testMisClass`.


## Step 4 - Naive Bayes

The next model is done using Naive Bayes.

The confusion matrix for the training data:

```{r}
naive.model = naiveBayes(good_bad ~ ., data=train)
naive.model$levels<-c('bad', 'good')

#Sets training data for naive
naive.train = predict(naive.model, newdata=train)
confMa.naive.train = table(naive.train, train$good_bad)
misCl.naive.train = 1-(sum(diag(confMa.naive.train))/sum(confMa.naive.train))
knitr::kable(confMa.naive.train, format = 'latex')

```

The missclassification rate for the training data is `r misCl.naive.train`.

The confusion matrix for the test data:

``` {r}
#Sets test for naive
naive.test = predict(naive.model, newdata=test)
confMa.naive.test = table(naive.test, test$good_bad)
misCl.naive.test = 1-(sum(diag(confMa.naive.test))/sum(confMa.naive.test))
knitr::kable(confMa.naive.test, format = 'latex')

```

Also, the missclassification rate for the test data is `r misCl.naive.test`.

As we only acquired the test data from step 3 this is the only data we have to compare to. What can be seen is that the missclassification rate is a bit worse that for the tree in step 3.

## Step 5 - ROC

```{r}
pi = seq(0.05, 0.95, by=0.05)
tree.model.dataframe = data.frame("0.05" = c(rep(0,250)))
tree.model.tpr = c(rep(0,length(pi)))
tree.model.fpr = c(rep(0,length(pi)))

for(k in 1:length(pi)) {
  tree.model.dataframe[, k] = ifelse(predict.tree > pi[k], 1, 0)
  confMa.tmp = table(ifelse(test$good_bad == 'good', 1, 0), tree.model.dataframe[, k])
  if (dim(confMa.tmp)[2] == 1) {
    if (colnames(confMa.tmp) == "1") {
      confMa.tmp <- cbind(as.matrix(c(0, 0)), confMa.tmp)
    } else {
      confMa.tmp <- cbind(confMa.tmp, as.matrix(c(0, 0)))
    }
  }
  
  tree.model.tpr[k] = confMa.tmp[2, 2] / (confMa.tmp[2, 1] + confMa.tmp[2, 2])
  tree.model.fpr[k] = confMa.tmp[1, 2] / (confMa.tmp[1, 1] + confMa.tmp[1, 2])
  
}
plot(x = tree.model.fpr, y = tree.model.tpr, type = "l", 
     col="green", main="ROC Curve", sub="Green = tree, Blue = naive", xlab="FPR", ylab="TPR")


set.seed(12345)
#ROC for naive
naive.test.raw = predict(naive.model, newdata = test, type="raw")

naive.model.dataframe = data.frame("0.05" = c(rep(0,250)))
naive.model.tpr = c(rep(0,length(pi)))
naive.model.fpr = c(rep(0,length(pi)))


for (k in 1:length(pi)) {
  naive.model.dataframe[, k] = ifelse(naive.test.raw[, 2] > pi[k], 1, 0)
  confMa.tmp = table(ifelse(test$good_bad == 'good', 1, 0), naive.model.dataframe[, k])
  
  if (dim(confMa.tmp)[2] == 1) {
    if (colnames(confMa.tmp) == "1") {
      confMa.tmp <- cbind(as.matrix(c(0, 0)), confMa.tmp)
    } else {
      confMa.tmp <- cbind(confMa.tmp, as.matrix(c(0, 0)))
    }
  }
  
  naive.model.tpr[k] = confMa.tmp[2, 2] / (confMa.tmp[2, 1] + confMa.tmp[2, 2])
  naive.model.fpr[k] = confMa.tmp[1, 2] / (confMa.tmp[1, 1] + confMa.tmp[1, 2])
  
}
lines(x = naive.model.fpr, y = naive.model.tpr, col="blue")
```

In the graph above one can see the ROC curves for the tree-model and for the Naive Bayes-model. They differ in the AUC and the Naive Model has a larger AUC compared to the tree-model. What this means is that the best classifier is the Naive Bayes model as it has the greatest AUC.


## Step 6 - Loss matrix with Naive Bayes

```{r}
naive.train.raw = predict(naive.model, newdata = train, type="raw")

#1*p(bad|x) > 10*p(good|x) -> bad, L12=1, L21=10, L12/L21=10
confMa.train.naive.loss = table(train$good_bad, ifelse(
                          naive.train.raw[,1]/naive.train.raw[,2] > 0.1, "bad", "good"))
misCl.train.naive.loss <- 1-(sum(diag(confMa.train.naive.loss))/sum(confMa.train.naive.loss))

confMa.test.naive.loss = table(test$good_bad, ifelse(naive.test.raw[,1]/naive.test.raw[,2] > 0.1, "bad", "good"))

misCl.test.naive.loss <- 1-(sum(diag(confMa.test.naive.loss))/sum(confMa.test.naive.loss))

```

The table for the confusion matrix of the training data for the Naive Bayes model model: 

```{r}
knitr::kable(confMa.train.naive.loss, format = 'latex')

```

The training data also has the missclassification rate of `r misCl.train.naive.loss`.

The table for the confusion matrix of the test data for the Naive Bayes model model: 

```{r}
knitr::kable(confMa.test.naive.loss, format = 'latex')

```

The test data also has the missclassification rate of `r misCl.test.naive.loss`.

Firstly, what can be seen is that the missclassification rate with the loss is higher than without it. The reason for this is that the loss matrix makes it so that it is a lot worse to give loan to a "bad" customer than to not give a "good" customer. Secondly, one can also see that clearly in the loss matrix with the lowest value being giving a loan to a bad customer.


\newpage

# Assignment 4
```{r, code=readLines("Ass 4.R"), include=FALSE}
```
This assignment included two types of analysis, principal component analysis (PCA) and independent component analysis (ICA), of measured spectra for different diesel fuels to predict the viscosity level of the fuel.

## Step 1 - Principal Component Analysis

```{r echo=FALSE}
screeplot(pca.spectra, main = "Screeplot of PCA")
```
By the plot it could interpreted to only need to extract two principal components. The minimal number of components for explaining 99 % of the total variance - PC1 and PC2 explains 99.6%.

```{r echo=FALSE}
plot(pca.spectra$x[,1], pca.spectra$x[,2],main = "PCA plot", xlab="PC1", ylab="PC2")
```
According to this plot there are 2 "unique" diesel fuels and a group of 5 dieselfuels which differ from the majority by PC1.

## Step 2 - Trace plot of PC1 and PC2
```{r echo=FALSE}
plot(pca.spectra_rot[,1], main="Traceplot - PC1", ylab = "Rotation of PC1")
plot(pca.spectra_rot[,2], main="Traceplot - PC2", ylab = "Rotation of PC2")
```
By the looks of the trace plots, PC2 is mainly explained by the last 10 spectra. PC1 is explained by the first 50 %.

## Step 3 - Independent Component Analysis

$W ^\prime = K \cdot W$

```{r echo=FALSE}
plot(ica.spectra_Winv[,1], main="Traceplot - W' Col 1", ylab = "W'(Col1)")
plot(ica.spectra_Winv[,2], main="Traceplot - W' Col 2", ylab = "W'(Col1)")
```
When comparing these traceplots to the traceplots of step 2, the traceplot of Wâ€™ Col 2 is very similar to the plot of PC2 with inverted y-axis.

The measure represented by the matrix $W ^\prime$ may be interpreted as a whitening un-mixing matrix. This is because it is supposed to be the invers, and undo the work, of the mixing matrix. 

```{r echo=FALSE}
plot(ica.spectra$S[,1], ica.spectra$S[,2], main = "Score", ylab = "Latent 2", xlab = "Latent 1")
```
This plot, of the correlation of the two latent features,is quite similar to the plot of PC1 and PC2 in step 1 - with the difference of being rotated 180 degrees.

\newpage

# Appendices

## Appendix I - Assignment 1

```{r echo=TRUE, warning=FALSE, eval=FALSE}
#setup
#RNGversion('3.5.1')
set.seed(12345)
library(ggplot2)
library(MASS)

#1
crabs.data <- read.csv("australian-crabs.csv")
crabs.sex.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=sex)) + geom_point()
crabs.sex.plot

#2
crabs.sex.lda_model <- lda(sex ~ RW + CL, data = crabs.data)
print(crabs.sex.lda_model)
crabs.predicted <- predict(crabs.sex.lda_model,crabs.data, prior = crabs.sex.lda_model$prior)
crabs.predicted.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=crabs.predicted$class))+ geom_point()
crabs.predicted.plot
crabs.predicted.confusionmatrix <- table(crabs.predicted$class,crabs.data$sex)
crabs.predicted.confusionmatrix
crabs.predicted.missclassification_rate <- 1 - (sum(diag(crabs.predicted.confusionmatrix))/sum(crabs.predicted.confusionmatrix))
crabs.predicted.missclassification_rate

#3
crabs.sex.own_priors <- crabs.sex.lda_model$prior
crabs.sex.own_priors <- c(0.1,0.9)
crabs.sex.own_priors.predicted <- predict(crabs.sex.lda_model,crabs.data, prior = crabs.sex.own_priors)
crabs.sex.own_priors.predicted.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=crabs.sex.own_priors.predicted$class)) + geom_point()
crabs.sex.own_priors.predicted.plot

crabs.sex.own_priors.predicted.confusionmatrix <- table(crabs.sex.own_priors.predicted$class,crabs.data$sex)
crabs.sex.own_priors.predicted.confusionmatrix
crabs.sex.own_priors.predicted.missclassification_rate <- 1- (sum(diag(crabs.sex.own_priors.predicted.confusionmatrix))/sum(crabs.sex.own_priors.predicted.confusionmatrix))
crabs.sex.own_priors.predicted.missclassification_rate

#4
crabs.sex.glm_model <- glm(sex ~ RW + CL, family=binomial, data = crabs.data)
crabs.sex.glm_model.predicted <- predict(crabs.sex.glm_model,crabs.data, type="response")
crabs.sex.glm_model.classified <- ifelse(crabs.sex.glm_model.predicted > 0.5, "Male", "Female")

crabs.sex.glm_model.plot <- ggplot(crabs.data, aes(x=CL, y=RW, color=crabs.sex.glm_model.classified)) + geom_point()

# Extracting coefficients for line equation
line.B = crabs.sex.glm_model[["coefficients"]][["CL"]]
line.A = crabs.sex.glm_model[["coefficients"]][["RW"]]
line.M = crabs.sex.glm_model[["coefficients"]][["(Intercept)"]]
line.limit = 0 # From the equation of a straight line
# Creating function from line equation to use for plot
line.equ <- function(x){(line.B*x+line.M-line.limit)/(-line.A)}

crabs.sex.glm_model.plot <- crabs.sex.glm_model.plot + geom_line(aes(x = CL, y = line.equ(CL), color = "Decision\nboundry")) + labs(title="Crabs classified", color = "Legend")

crabs.sex.glm_model.plot


crabs.sex.glm_model.predicted.confusionmatrix <- table(crabs.sex.glm_model.classified,crabs.data$sex)
crabs.sex.glm_model.predicted.confusionmatrix
crabs.sex.glm_model.predicted.missclassification_rate <- 1- (sum(diag(crabs.sex.glm_model.predicted.confusionmatrix))/sum(crabs.sex.glm_model.predicted.confusionmatrix))
crabs.sex.glm_model.predicted.missclassification_rate

```



\newpage

## Appendix II - Assignment 2

```{r echo=TRUE, warning=FALSE, eval=FALSE}
library(tree)
library(readxl)
library(MASS)
library(e1071)

data <- read_excel("creditscoring.xls")
set.seed(12345)

#Splits data into training, validation and test
n=dim(data)[1]
id=sample(1:n, floor(n*0.5))
train=data[id,]
id1=setdiff(1:n, id)
set.seed(12345)
id2=sample(id1, floor(n*0.25))
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]


#Train and test for deviance
fit.train.dev <- tree(ifelse(good_bad == "good", 1, 0) ~ . , data=train, split="deviance")

#Train and test for gini
fit.train.gini <- tree(ifelse(good_bad == "good", 1, 0)  ~ . , data=train, split="gini")

#Predictions
fit_pred.train.dev <- predict(fit.train.dev, newdata=train)

fit_pred.train.gini <- predict(fit.train.gini, newdata=train)

fit_pred.test.dev <- predict(fit.train.dev, newdata=test)

fit_pred.test.gini <- predict(fit.train.gini, newdata=test)

#Conf matrix
confMa.train.dev <- table(train$good_bad,(ifelse(fit_pred.train.dev > 0.5, "good", "bad")))
confMa.test.dev <- table(test$good_bad,(ifelse(fit_pred.test.dev > 0.5, "good", "bad")))
confMa.train.gini <- table(train$good_bad,(ifelse(fit_pred.train.gini > 0.5, "good", "bad")))
confMa.test.gini <- table(test$good_bad,(ifelse(fit_pred.test.gini > 0.5, "good", "bad")))

#Misclassification rates
misCl.train.dev <- 1-(sum(diag(confMa.train.dev))/sum(confMa.train.dev))
misCl.test.dev <- 1-(sum(diag(confMa.test.dev))/sum(confMa.test.dev))
misCl.train.gini <- 1-(sum(diag(confMa.train.gini))/sum(confMa.train.gini))
misCl.test.gini <- 1-(sum(diag(confMa.test.gini))/sum(confMa.test.gini))

#Prints the miscl. rates
misCl.train.dev
misCl.test.dev
misCl.train.gini
misCl.test.gini

#I choose dev as it has the best miscl-rate

#3 
trainScore=rep(0,9)
validationScore=rep(0,9)

for (i in 2:9) {
  prunedTree <- prune.tree(fit.train.dev, best=i)
  prediction <- predict(prunedTree, newdata = valid, type = "tree") # Predict with the pruned tree and val set
  trainScore[i] <- deviance(prunedTree) # Calculate deviance of test set
  validationScore[i] <- deviance(prediction) # Calculate deviance of val set
}

plot(2:9, trainScore[2:9], type="b", col="green", ylim=c(40,100), ylab="Deviance", xlab="No. of leaves")
points(2:9, validationScore[2:9], type="b", col="blue", ylim=c(40,100))
legend("top", legend=c("Training score (green)", "Validation score (blue)"))

#Optimal number of leaves
match(min(validationScore[2:9]),validationScore)
optPrunedTree <- prune.tree(fit.train.dev, best=match(min(validationScore[2:9]),validationScore))
plot(optPrunedTree, sub="asd")
text(optPrunedTree, pretty=0)
title("Pruned tree with 6 leaves")

#Estimates classification 
predict.tree <- predict(optPrunedTree, newdata = test)
confMa.prune <- table(ifelse(predict.tree>0.5, "good", "bad"), test$good_bad)
testMisClass <- 1-sum(diag(confMa.prune)/sum(confMa.prune))
print(testMisClass)


#4
set.seed(12345)


naive.model = naiveBayes(good_bad ~ ., data=train)
naive.model$levels<-c('bad', 'good')

#Sets training data for naive
set.seed(12345)
naive.train = predict(naive.model, newdata=train)
confMa.naive.train = table(naive.train, train$good_bad)
misCl.naive.train = 1-(sum(diag(confMa.naive.train))/sum(confMa.naive.train))
print(confMa.naive.train)
print(misCl.naive.train)

#Sets test for naive
set.seed(12345)
naive.test = predict(naive.model, newdata=test)
confMa.naive.test = table(naive.test, test$good_bad)
misCl.naive.test = 1-(sum(diag(confMa.naive.test))/sum(confMa.naive.test))
print(confMa.naive.test)
print(misCl.naive.test)


#5
pi = seq(0.05, 0.95, by=0.05)
tree.model.dataframe = data.frame("0.05" = c(rep(0,250)))
tree.model.tpr = c(rep(0,length(pi)))
tree.model.fpr = c(rep(0,length(pi)))

for(k in 1:length(pi)) {
  tree.model.dataframe[, k] = ifelse(predict.tree > pi[k], 1, 0)
  confMa.tmp = table(ifelse(test$good_bad == 'good', 1, 0), tree.model.dataframe[, k])
  print(confMa.tmp)
  
  
  
  if (dim(confMa.tmp)[2] == 1) {
    if (colnames(confMa.tmp) == "1") {
      confMa.tmp <- cbind(as.matrix(c(0, 0)), confMa.tmp)
    } else {
      confMa.tmp <- cbind(confMa.tmp, as.matrix(c(0, 0)))
    }
  }
  
  tree.model.tpr[k] = confMa.tmp[2, 2] / (confMa.tmp[2, 1] + confMa.tmp[2, 2])
  tree.model.fpr[k] = confMa.tmp[1, 2] / (confMa.tmp[1, 1] + confMa.tmp[1, 2])
  
}
plot(x = tree.model.fpr, y = tree.model.tpr, type = "l", col="green", main="Green = tree, Blue = naive")


set.seed(12345)
#ROC for naive
naive.test.raw = predict(naive.model, newdata = test, type="raw")

naive.model.dataframe = data.frame("0.05" = c(rep(0,250)))
naive.model.tpr = c(rep(0,length(pi)))
naive.model.fpr = c(rep(0,length(pi)))


for (k in 1:length(pi)) {
  naive.model.dataframe[, k] = ifelse(naive.test.raw[, 2] > pi[k], 1, 0)
  confMa.tmp = table(ifelse(test$good_bad == 'good', 1, 0), naive.model.dataframe[, k])
  
  if (dim(confMa.tmp)[2] == 1) {
    if (colnames(confMa.tmp) == "1") {
      confMa.tmp <- cbind(as.matrix(c(0, 0)), confMa.tmp)
    } else {
      confMa.tmp <- cbind(confMa.tmp, as.matrix(c(0, 0)))
    }
  }
  
  naive.model.tpr[k] = confMa.tmp[2, 2] / (confMa.tmp[2, 1] + confMa.tmp[2, 2])
  naive.model.fpr[k] = confMa.tmp[1, 2] / (confMa.tmp[1, 1] + confMa.tmp[1, 2])
  
}
lines(x = naive.model.fpr, y = naive.model.tpr, col="blue")

#6 
naive.train.raw = predict(naive.model, newdata = train, type="raw")


#1*p(bad|x) > 10*p(good|x) -> bad, L12=1, L21=10, L12/L21=10
confMa.train.naive.loss = table(train$good_bad, ifelse(naive.train.raw[,1]/naive.train.raw[,2] > 0.1, "bad", "good"))
print(confMa.train.naive.loss)
misCl.train.naive.loss <- 1-(sum(diag(confMa.train.naive.loss))/sum(confMa.train.naive.loss))
print(misCl.train.naive.loss)

confMa.test.naive.loss = table(test$good_bad, ifelse(naive.test.raw[,1]/naive.test.raw[,2] > 0.1, "bad", "good"))


print(confMa.test.naive.loss)
misCl.test.naive.loss <- 1-(sum(diag(confMa.test.naive.loss))/sum(confMa.test.naive.loss))
print(misCl.test.naive.loss)

```


\newpage

## Appendix III - Assignment 4
```{r, code=readLines("Ass 4.R"), eval=FALSE, echo=TRUE}
```